{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfe8ba5f83f2d38c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 16:57:37.948917: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-19 16:57:37.975019: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-19 16:57:37.975050: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-19 16:57:37.976123: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-19 16:57:37.981668: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-19 16:57:38.680031: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from model.encoder import Encoder\n",
    "from model.decoder import Decoder\n",
    "from model.tcvae import TCVAE\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from keras.src.optimizers import RMSprop\n",
    "from src.utils.helper import Helper\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce1f5a18-cccd-481a-bb62-537efec0ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TFDS_DATA_DIR'] = '/mnt/sdb/home/ml/tensorflow_datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7ca4316-c961-4aa4-91d3-ce76a48320e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model, datasets):\n",
    "    split = datasets['split']\n",
    "    batch_size = datasets['batch_size']\n",
    "    result = []\n",
    "    for dataset in datasets['name']:\n",
    "        data_train = tfds.load(dataset, split=[split])\n",
    "        train = data_train[0].batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        labels = Helper.get_labels(train)\n",
    "\n",
    "        df = model._encoder.predict(Helper.data_generator([train], method='stop'))\n",
    "        df = df[0]  # reparameterize(df[0], df[1]).numpy()\n",
    "        ld = df.shape[1]\n",
    "\n",
    "        labels.index = range(0, len(labels))\n",
    "        df = pd.concat([pd.DataFrame(df), labels], axis=1)\n",
    "        result.append(df)\n",
    "\n",
    "    return result, ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c209f9c-b047-48fb-b5f8-54655ac7759b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 16:57:41.969320: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-19 16:57:42.056207: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-19 16:57:42.056456: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-19 16:57:42.057666: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-19 16:57:42.057798: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-19 16:57:42.057901: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-19 16:57:42.137889: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-19 16:57:42.138044: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-19 16:57:42.138188: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-19 16:57:42.138283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 896 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_9684913) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('../results_round_3/2024-02-16_15-25-04/model_best/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2f9e80f-bf3e-42cd-99db-a92a3a3d7b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'name': ['icentia11k'],\n",
    "    'shuffle_size': 1024,\n",
    "    'batch_size': 1024,\n",
    "}\n",
    "\n",
    "splits = [\n",
    "    '107', '5484', '6998', '3984', '3111', '4040', '3013', '6607', '4219', '8750', '5665', '9225',\n",
    "    '8030', '9886', '1851', '1123', '3043', '3369', '6829', '10969', '3088', '9405', '9535', '4993',\n",
    "    '4209', '10937', '6167', '4688', '6877', '10733', '8412', '10146', '10973', '9345', '2514', '2908',\n",
    "    '5938', '5015', '9595', '8769', '4786', '2602', '7779', '2826', '1118', '3485', '2980', '10503',\n",
    "    '7719', '6575', '1722', '7234', '8366', '3948', '5493', '10731', '8111', '2820', '5337', '5369',\n",
    "    '4184', '9403', '9625', '303', '33', '3274', '1941', '9116', '9283', '3522', '4836', '7107', '251',\n",
    "    '9071', '6899', '9733', '9440', '457', '2954', '1839', '5865', '8500', '9559', '1277', '1145', '10107',\n",
    "    '9287', '8443', '9783', '9956', '10090', '3204', '6814', '4553', '6377', '5572', '1178', '5032', '1793', '4453',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4befe89f-0278-4acc-91da-6b425b5aecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ld = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6094bf7-aaa2-47bc-b10d-595c59c85d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t Split: 107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 16:57:49.657483: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2024-02-19 16:57:49.702110: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-02-19 16:57:49.894603: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "236/236 [==============================] - 17s 60ms/step - loss: 7.3040 - recon: 2.1283 - kl_loss: 6.4109 - mi: 4.3391 - tc: 1.6345 - dw_kl: 15.0013 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 2/20\n",
      "236/236 [==============================] - 12s 53ms/step - loss: 7.3274 - recon: 2.0788 - kl_loss: 6.5136 - mi: 4.3800 - tc: 1.9071 - dw_kl: 14.2350 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 3/20\n",
      "236/236 [==============================] - 13s 55ms/step - loss: 7.5491 - recon: 2.0333 - kl_loss: 6.6002 - mi: 4.5648 - tc: 2.1485 - dw_kl: 14.4205 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 4/20\n",
      "236/236 [==============================] - 22s 60ms/step - loss: 7.6282 - recon: 2.0606 - kl_loss: 57.7684 - mi: 4.5350 - tc: 2.0930 - dw_kl: 14.9310 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 5/20\n",
      "236/236 [==============================] - 12s 51ms/step - loss: 7.2402 - recon: 2.0070 - kl_loss: 6.6872 - mi: 4.4571 - tc: 2.0051 - dw_kl: 13.6886 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 6/20\n",
      "236/236 [==============================] - 14s 57ms/step - loss: 7.6345 - recon: 2.0402 - kl_loss: 6.6721 - mi: 4.6516 - tc: 2.0218 - dw_kl: 15.2325 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 7/20\n",
      "236/236 [==============================] - 12s 52ms/step - loss: 6.6189 - recon: 2.1148 - kl_loss: 6.6813 - mi: 4.4723 - tc: 1.2036 - dw_kl: 13.2343 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 8/20\n",
      "236/236 [==============================] - 22s 57ms/step - loss: 7.3757 - recon: 2.0474 - kl_loss: 6.7181 - mi: 4.3664 - tc: 1.8851 - dw_kl: 14.7346 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 9/20\n",
      "236/236 [==============================] - 13s 55ms/step - loss: 7.8262 - recon: 2.0567 - kl_loss: 6.7154 - mi: 4.5951 - tc: 1.8751 - dw_kl: 16.7520 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 10/20\n",
      "236/236 [==============================] - 22s 60ms/step - loss: 7.4414 - recon: 1.9925 - kl_loss: 6.7067 - mi: 4.6367 - tc: 1.6910 - dw_kl: 15.8438 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 11/20\n",
      "236/236 [==============================] - 21s 87ms/step - loss: 7.4958 - recon: 2.0243 - kl_loss: 6.7259 - mi: 4.7050 - tc: 1.8715 - dw_kl: 15.1666 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 12/20\n",
      "236/236 [==============================] - 20s 85ms/step - loss: 7.6984 - recon: 1.9474 - kl_loss: 6.7206 - mi: 4.6047 - tc: 2.0481 - dw_kl: 15.9584 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 13/20\n",
      "236/236 [==============================] - 13s 55ms/step - loss: 7.0005 - recon: 2.0620 - kl_loss: 6.7241 - mi: 4.5293 - tc: 1.3772 - dw_kl: 14.6546 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 14/20\n",
      "236/236 [==============================] - 13s 57ms/step - loss: 7.2120 - recon: 2.0941 - kl_loss: 6.7311 - mi: 4.4821 - tc: 1.5537 - dw_kl: 14.8922 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 15/20\n",
      "236/236 [==============================] - 14s 57ms/step - loss: 7.5257 - recon: 2.0469 - kl_loss: 6.7424 - mi: 4.6642 - tc: 1.8844 - dw_kl: 15.1924 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 16/20\n",
      "236/236 [==============================] - 14s 57ms/step - loss: 7.2314 - recon: 2.1025 - kl_loss: 6.7460 - mi: 4.5167 - tc: 1.7162 - dw_kl: 14.2629 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 17/20\n",
      "236/236 [==============================] - 13s 53ms/step - loss: 7.2323 - recon: 2.0091 - kl_loss: 6.7341 - mi: 4.7495 - tc: 1.6464 - dw_kl: 14.7810 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 18/20\n",
      "236/236 [==============================] - 13s 55ms/step - loss: 7.6865 - recon: 2.0080 - kl_loss: 6.7493 - mi: 4.6887 - tc: 1.9970 - dw_kl: 15.7155 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 19/20\n",
      "236/236 [==============================] - 14s 57ms/step - loss: 7.1632 - recon: 2.0940 - kl_loss: 6.7523 - mi: 4.6003 - tc: 1.6752 - dw_kl: 14.0447 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 20/20\n",
      "236/236 [==============================] - 21s 61ms/step - loss: 7.4246 - recon: 2.0269 - kl_loss: 6.7507 - mi: 4.4854 - tc: 2.0716 - dw_kl: 14.2163 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "236/236 [==============================] - 13s 54ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 17:03:40.366415: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8095925380998157109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \t Split: 5484\n",
      "Epoch 1/20\n",
      "302/302 [==============================] - 19s 60ms/step - loss: 27.2065 - recon: 20.9644 - kl_loss: 6.9740 - mi: 3.8055 - tc: 1.4778 - dw_kl: 21.4936 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 2/20\n",
      "302/302 [==============================] - 18s 54ms/step - loss: 24.4868 - recon: 18.0501 - kl_loss: 6.3420 - mi: 3.8683 - tc: 1.5410 - dw_kl: 22.1514 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 3/20\n",
      "302/302 [==============================] - 18s 58ms/step - loss: 24.0392 - recon: 17.8918 - kl_loss: 6.1695 - mi: 3.9683 - tc: 1.6663 - dw_kl: 20.1036 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 4/20\n",
      "302/302 [==============================] - 16s 54ms/step - loss: 24.7693 - recon: 18.3530 - kl_loss: 6.0981 - mi: 3.9495 - tc: 1.7497 - dw_kl: 21.1330 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 5/20\n",
      "302/302 [==============================] - 22s 59ms/step - loss: 23.9878 - recon: 17.7990 - kl_loss: 6.0578 - mi: 4.0196 - tc: 1.7494 - dw_kl: 19.9270 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 6/20\n",
      "302/302 [==============================] - 17s 55ms/step - loss: 23.7367 - recon: 17.2794 - kl_loss: 6.0096 - mi: 4.1397 - tc: 1.8581 - dw_kl: 20.7140 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 7/20\n",
      "302/302 [==============================] - 17s 57ms/step - loss: 23.0947 - recon: 17.1107 - kl_loss: 5.9640 - mi: 4.0604 - tc: 1.7086 - dw_kl: 19.0252 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 8/20\n",
      "302/302 [==============================] - 21s 57ms/step - loss: 23.7234 - recon: 17.4745 - kl_loss: 5.9269 - mi: 4.1358 - tc: 1.7582 - dw_kl: 20.0760 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 9/20\n",
      "302/302 [==============================] - 20s 57ms/step - loss: 23.5949 - recon: 17.5280 - kl_loss: 5.8845 - mi: 4.0963 - tc: 1.7384 - dw_kl: 19.2848 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 10/20\n",
      "302/302 [==============================] - 21s 58ms/step - loss: 23.4555 - recon: 17.3974 - kl_loss: 5.8591 - mi: 4.2194 - tc: 1.6651 - dw_kl: 19.4106 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 11/20\n",
      "302/302 [==============================] - 21s 58ms/step - loss: 23.0019 - recon: 16.9552 - kl_loss: 5.8298 - mi: 4.1529 - tc: 1.7087 - dw_kl: 19.2457 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 12/20\n",
      "302/302 [==============================] - 17s 56ms/step - loss: 22.7128 - recon: 16.5867 - kl_loss: 5.8103 - mi: 4.2673 - tc: 1.6591 - dw_kl: 19.7270 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 13/20\n",
      "302/302 [==============================] - 15s 38ms/step - loss: 22.9660 - recon: 16.7931 - kl_loss: 5.8002 - mi: 4.2794 - tc: 1.7792 - dw_kl: 19.4682 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 14/20\n",
      "302/302 [==============================] - 12s 39ms/step - loss: 22.6171 - recon: 16.6861 - kl_loss: 5.7844 - mi: 4.1633 - tc: 1.6121 - dw_kl: 19.0433 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 15/20\n",
      "302/302 [==============================] - 12s 39ms/step - loss: 23.0170 - recon: 16.8095 - kl_loss: 5.8009 - mi: 4.1410 - tc: 1.6916 - dw_kl: 20.1303 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 16/20\n",
      "302/302 [==============================] - 12s 39ms/step - loss: 22.7837 - recon: 16.7997 - kl_loss: 6.2189 - mi: 4.1534 - tc: 1.6667 - dw_kl: 19.0999 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 17/20\n",
      "302/302 [==============================] - 21s 40ms/step - loss: 22.2204 - recon: 16.0466 - kl_loss: 5.7715 - mi: 4.3208 - tc: 1.6246 - dw_kl: 20.0499 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 18/20\n",
      "302/302 [==============================] - 12s 40ms/step - loss: 22.3318 - recon: 16.3146 - kl_loss: 5.7657 - mi: 4.2541 - tc: 1.6006 - dw_kl: 19.4294 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 19/20\n",
      "302/302 [==============================] - 21s 41ms/step - loss: 22.3525 - recon: 16.4255 - kl_loss: 5.7561 - mi: 4.2163 - tc: 1.6392 - dw_kl: 18.8623 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 20/20\n",
      "302/302 [==============================] - 12s 39ms/step - loss: 22.5896 - recon: 16.6871 - kl_loss: 5.7430 - mi: 4.2631 - tc: 1.5955 - dw_kl: 18.8676 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "302/302 [==============================] - 13s 42ms/step\n",
      "2 \t Split: 6998\n",
      "Epoch 1/20\n",
      "247/247 [==============================] - 15s 58ms/step - loss: 33.9700 - recon: 26.8829 - kl_loss: 6.7735 - mi: 3.9966 - tc: 1.8318 - dw_kl: 24.1117 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 11s 46ms/step - loss: 32.1903 - recon: 25.6422 - kl_loss: 6.5995 - mi: 4.0607 - tc: 1.7157 - dw_kl: 21.8170 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 10s 39ms/step - loss: 32.3908 - recon: 25.9970 - kl_loss: 6.5684 - mi: 3.9541 - tc: 1.5419 - dw_kl: 21.8470 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 10s 39ms/step - loss: 31.9671 - recon: 25.3579 - kl_loss: 6.6215 - mi: 4.0588 - tc: 1.6536 - dw_kl: 22.3722 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 5/20\n",
      "247/247 [==============================] - 10s 39ms/step - loss: 31.2198 - recon: 24.5694 - kl_loss: 6.6660 - mi: 4.1949 - tc: 1.6420 - dw_kl: 22.4892 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 6/20\n",
      "247/247 [==============================] - 10s 39ms/step - loss: 32.3399 - recon: 25.9944 - kl_loss: 6.6833 - mi: 4.1568 - tc: 1.4983 - dw_kl: 21.5772 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 7/20\n",
      "247/247 [==============================] - 10s 39ms/step - loss: 30.7351 - recon: 24.3971 - kl_loss: 6.6976 - mi: 4.1864 - tc: 1.5977 - dw_kl: 21.1129 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 8/20\n",
      "247/247 [==============================] - 10s 39ms/step - loss: 30.7404 - recon: 23.8341 - kl_loss: 6.7135 - mi: 4.2561 - tc: 1.7245 - dw_kl: 23.3772 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 9/20\n",
      "247/247 [==============================] - 10s 39ms/step - loss: 30.4851 - recon: 23.8929 - kl_loss: 6.7036 - mi: 4.1845 - tc: 1.5920 - dw_kl: 22.4089 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 10/20\n",
      "247/247 [==============================] - 10s 39ms/step - loss: 30.8301 - recon: 24.4385 - kl_loss: 6.7048 - mi: 4.2164 - tc: 1.6141 - dw_kl: 21.2854 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 11/20\n",
      "247/247 [==============================] - 10s 39ms/step - loss: 32.7394 - recon: 26.4448 - kl_loss: 6.7054 - mi: 4.2083 - tc: 1.6332 - dw_kl: 20.7320 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 12/20\n",
      "247/247 [==============================] - 10s 38ms/step - loss: 30.3296 - recon: 23.8016 - kl_loss: 6.7222 - mi: 4.2377 - tc: 1.6604 - dw_kl: 21.7604 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 13/20\n",
      "247/247 [==============================] - 10s 38ms/step - loss: 32.4402 - recon: 26.1568 - kl_loss: 6.7117 - mi: 4.1822 - tc: 1.5599 - dw_kl: 20.9955 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 14/20\n",
      "247/247 [==============================] - 9s 38ms/step - loss: 31.7539 - recon: 25.5441 - kl_loss: 6.7184 - mi: 4.1721 - tc: 1.6147 - dw_kl: 20.4184 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 15/20\n",
      "247/247 [==============================] - 10s 39ms/step - loss: 30.7076 - recon: 24.0616 - kl_loss: 6.7082 - mi: 4.2229 - tc: 1.6150 - dw_kl: 22.5469 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 16/20\n",
      "247/247 [==============================] - 9s 36ms/step - loss: 30.6622 - recon: 24.0116 - kl_loss: 6.7171 - mi: 4.2744 - tc: 1.6342 - dw_kl: 22.4417 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 17/20\n",
      "247/247 [==============================] - 10s 39ms/step - loss: 29.8629 - recon: 23.4485 - kl_loss: 6.7109 - mi: 4.2103 - tc: 1.6121 - dw_kl: 21.4136 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 18/20\n",
      "247/247 [==============================] - 9s 37ms/step - loss: 30.5976 - recon: 23.7874 - kl_loss: 6.7085 - mi: 4.3221 - tc: 1.6565 - dw_kl: 23.1027 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 19/20\n",
      "247/247 [==============================] - 9s 38ms/step - loss: 29.8449 - recon: 23.2192 - kl_loss: 6.7176 - mi: 4.2698 - tc: 1.6554 - dw_kl: 22.2371 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 20/20\n",
      "247/247 [==============================] - 10s 39ms/step - loss: 30.3183 - recon: 23.4795 - kl_loss: 6.7132 - mi: 4.2929 - tc: 1.7030 - dw_kl: 23.0891 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "247/247 [==============================] - 9s 38ms/step\n",
      "3 \t Split: 3984\n",
      "Epoch 1/20\n",
      "203/203 [==============================] - 10s 40ms/step - loss: 26.7243 - recon: 21.6326 - kl_loss: 4.8001 - mi: 3.3452 - tc: 1.1252 - dw_kl: 17.6123 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 2/20\n",
      "203/203 [==============================] - 7s 35ms/step - loss: 26.7042 - recon: 21.7251 - kl_loss: 5.0256 - mi: 3.4572 - tc: 1.0729 - dw_kl: 17.1470 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 3/20\n",
      "203/203 [==============================] - 8s 39ms/step - loss: 25.2848 - recon: 20.2465 - kl_loss: 5.0512 - mi: 3.4902 - tc: 1.0126 - dw_kl: 17.6509 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 4/20\n",
      "203/203 [==============================] - 9s 35ms/step - loss: 25.5830 - recon: 20.6344 - kl_loss: 5.0587 - mi: 3.5392 - tc: 0.9734 - dw_kl: 17.3104 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 5/20\n",
      "203/203 [==============================] - 8s 37ms/step - loss: 2825466413056.0000 - recon: 429865.5312 - kl_loss: 49438728950932072.0000 - mi: 1.9172 - tc: 3.1902 - dw_kl: 14127329181696.0000 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 6/20\n",
      "203/203 [==============================] - 8s 39ms/step - loss: 123.1027 - recon: 33.5576 - kl_loss: 1833204526929.8701 - mi: 3.4806 - tc: 2.9710 - dw_kl: 432.3607 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 7/20\n",
      "203/203 [==============================] - 8s 39ms/step - loss: 90.5489 - recon: 21.4258 - kl_loss: 2347.4308 - mi: 3.5320 - tc: 2.5016 - dw_kl: 332.0773 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 8/20\n",
      "203/203 [==============================] - 8s 39ms/step - loss: 26.3172 - recon: 20.6438 - kl_loss: 415.5542 - mi: 3.4293 - tc: 2.1078 - dw_kl: 16.5063 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 9/20\n",
      "203/203 [==============================] - 8s 39ms/step - loss: 25.8626 - recon: 20.3398 - kl_loss: 5.6089 - mi: 3.6046 - tc: 1.8234 - dw_kl: 16.7161 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 10/20\n",
      "203/203 [==============================] - 8s 38ms/step - loss: 25.5382 - recon: 20.1441 - kl_loss: 21233.9813 - mi: 3.5577 - tc: 1.4132 - dw_kl: 17.7600 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 11/20\n",
      "203/203 [==============================] - 7s 35ms/step - loss: 25.4839 - recon: 20.4077 - kl_loss: 5.2257 - mi: 3.5957 - tc: 1.1187 - dw_kl: 17.3107 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 12/20\n",
      "203/203 [==============================] - 8s 38ms/step - loss: 25.2510 - recon: 19.9520 - kl_loss: 5.1609 - mi: 3.5868 - tc: 1.1259 - dw_kl: 18.4047 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 13/20\n",
      "203/203 [==============================] - 8s 38ms/step - loss: 25.0803 - recon: 20.0237 - kl_loss: 5.1433 - mi: 3.5899 - tc: 1.0694 - dw_kl: 17.4155 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 14/20\n",
      "203/203 [==============================] - 7s 35ms/step - loss: 25.4084 - recon: 20.3065 - kl_loss: 5.1511 - mi: 3.5818 - tc: 1.0833 - dw_kl: 17.5944 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 15/20\n",
      "203/203 [==============================] - 8s 39ms/step - loss: 25.4780 - recon: 20.2998 - kl_loss: 5.1602 - mi: 3.6564 - tc: 1.1229 - dw_kl: 17.7432 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 16/20\n",
      "203/203 [==============================] - 10s 39ms/step - loss: 25.1260 - recon: 20.1461 - kl_loss: 5.1552 - mi: 3.5732 - tc: 1.0114 - dw_kl: 17.2809 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 17/20\n",
      "203/203 [==============================] - 8s 39ms/step - loss: 25.3941 - recon: 20.3658 - kl_loss: 5.1763 - mi: 3.6108 - tc: 1.0849 - dw_kl: 17.1911 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 18/20\n",
      "203/203 [==============================] - 8s 40ms/step - loss: 24.6134 - recon: 19.6122 - kl_loss: 5.1627 - mi: 3.6640 - tc: 1.0571 - dw_kl: 17.1137 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 19/20\n",
      "203/203 [==============================] - 8s 39ms/step - loss: 24.6042 - recon: 19.3863 - kl_loss: 5.1667 - mi: 3.6570 - tc: 1.0399 - dw_kl: 18.2729 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 20/20\n",
      "203/203 [==============================] - 8s 39ms/step - loss: 24.7382 - recon: 19.5940 - kl_loss: 5.1736 - mi: 3.6369 - tc: 1.0978 - dw_kl: 17.6928 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "203/203 [==============================] - 8s 39ms/step\n",
      "4 \t Split: 3111\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function VAE.encode at 0x7fc6dc7830a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function VAE.encode at 0x7fc6dc7830a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function VAE.decode at 0x7fc6dc7836d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function VAE.decode at 0x7fc6dc7836d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "198/198 [==============================] - 10s 43ms/step - loss: 18.7288 - recon: 14.5467 - kl_loss: 4.1868 - mi: 2.8671 - tc: 1.0400 - dw_kl: 13.8829 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 2/20\n",
      "198/198 [==============================] - 8s 38ms/step - loss: 18.5235 - recon: 14.3573 - kl_loss: 4.3306 - mi: 3.1991 - tc: 0.8976 - dw_kl: 14.0415 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 3/20\n",
      "198/198 [==============================] - 10s 39ms/step - loss: 17.8916 - recon: 13.6633 - kl_loss: 4.3375 - mi: 3.1476 - tc: 0.8829 - dw_kl: 14.4621 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 4/20\n",
      "198/198 [==============================] - 8s 39ms/step - loss: 18.0265 - recon: 13.7477 - kl_loss: 4.3505 - mi: 3.2336 - tc: 0.9566 - dw_kl: 14.3339 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 5/20\n",
      "198/198 [==============================] - 7s 37ms/step - loss: 17.9968 - recon: 13.8365 - kl_loss: 4.3703 - mi: 3.1927 - tc: 0.8715 - dw_kl: 14.1229 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 6/20\n",
      "198/198 [==============================] - 8s 39ms/step - loss: 18.5001 - recon: 14.1504 - kl_loss: 4.3960 - mi: 3.2578 - tc: 0.9164 - dw_kl: 14.8248 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 7/20\n",
      "198/198 [==============================] - 8s 41ms/step - loss: 17.8104 - recon: 13.5630 - kl_loss: 4.4055 - mi: 3.3508 - tc: 0.9162 - dw_kl: 14.2215 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 8/20\n",
      "198/198 [==============================] - 11s 54ms/step - loss: 17.7555 - recon: 13.4586 - kl_loss: 4.4222 - mi: 3.2766 - tc: 0.8913 - dw_kl: 14.6427 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 9/20\n",
      "198/198 [==============================] - 11s 57ms/step - loss: 18.0960 - recon: 13.6116 - kl_loss: 4.4395 - mi: 3.3703 - tc: 0.9984 - dw_kl: 15.0584 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 10/20\n",
      "198/198 [==============================] - 17s 41ms/step - loss: 18.0953 - recon: 13.5715 - kl_loss: 4.4571 - mi: 3.3200 - tc: 0.9577 - dw_kl: 15.4685 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 11/20\n",
      "198/198 [==============================] - 8s 39ms/step - loss: 17.9214 - recon: 13.7118 - kl_loss: 4.4515 - mi: 3.2163 - tc: 0.8883 - dw_kl: 14.2782 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 12/20\n",
      "198/198 [==============================] - 8s 39ms/step - loss: 18.5098 - recon: 13.9610 - kl_loss: 4.4519 - mi: 3.3275 - tc: 1.0325 - dw_kl: 15.2868 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 13/20\n",
      "198/198 [==============================] - 10s 39ms/step - loss: 18.0312 - recon: 13.6171 - kl_loss: 4.4587 - mi: 3.3299 - tc: 0.9015 - dw_kl: 15.1348 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 14/20\n",
      "198/198 [==============================] - 8s 39ms/step - loss: 17.7350 - recon: 13.3360 - kl_loss: 4.4563 - mi: 3.3299 - tc: 0.8802 - dw_kl: 15.1443 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 15/20\n",
      "198/198 [==============================] - 8s 39ms/step - loss: 17.9482 - recon: 13.5078 - kl_loss: 4.4900 - mi: 3.3119 - tc: 0.9572 - dw_kl: 15.0615 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 16/20\n",
      "198/198 [==============================] - 8s 39ms/step - loss: 17.4225 - recon: 13.0506 - kl_loss: 4.4621 - mi: 3.3912 - tc: 0.9233 - dw_kl: 14.7752 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 17/20\n",
      "198/198 [==============================] - 8s 39ms/step - loss: 17.7818 - recon: 13.5199 - kl_loss: 23746.7885 - mi: 3.2054 - tc: 0.8903 - dw_kl: 14.5431 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 18/20\n",
      "198/198 [==============================] - 8s 39ms/step - loss: 17.7400 - recon: 13.1587 - kl_loss: 4.4939 - mi: 3.4925 - tc: 1.0322 - dw_kl: 15.2847 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 19/20\n",
      "198/198 [==============================] - 8s 39ms/step - loss: 17.8731 - recon: 13.3507 - kl_loss: 4.4785 - mi: 3.4513 - tc: 1.0171 - dw_kl: 15.0921 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 20/20\n",
      "198/198 [==============================] - 8s 39ms/step - loss: 17.8229 - recon: 13.2795 - kl_loss: 4.6010 - mi: 3.3056 - tc: 1.0743 - dw_kl: 15.1138 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "198/198 [==============================] - 8s 38ms/step\n",
      "5 \t Split: 4040\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function VAE.encode at 0x7fc6ac3fcdc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function VAE.encode at 0x7fc6ac3fcdc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function VAE.decode at 0x7fc6ac3fd360> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function VAE.decode at 0x7fc6ac3fd360> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "277/277 [==============================] - 13s 42ms/step - loss: 44.0263 - recon: 36.6664 - kl_loss: 7.6040 - mi: 4.7098 - tc: 2.4656 - dw_kl: 22.2269 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 2/20\n",
      "277/277 [==============================] - 11s 38ms/step - loss: 43.5250 - recon: 35.3915 - kl_loss: 7.5561 - mi: 4.8559 - tc: 2.6275 - dw_kl: 25.3010 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 3/20\n",
      "277/277 [==============================] - 11s 39ms/step - loss: 42.6513 - recon: 35.0308 - kl_loss: 7.5974 - mi: 4.7651 - tc: 2.4350 - dw_kl: 23.5971 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 4/20\n",
      "277/277 [==============================] - 11s 38ms/step - loss: 43.7728 - recon: 36.3876 - kl_loss: 7.6399 - mi: 4.7562 - tc: 2.5579 - dw_kl: 21.9384 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 5/20\n",
      "277/277 [==============================] - 11s 39ms/step - loss: 43.2858 - recon: 35.6368 - kl_loss: 7.6595 - mi: 4.7252 - tc: 2.6822 - dw_kl: 22.7908 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 6/20\n",
      "277/277 [==============================] - 11s 38ms/step - loss: 42.5797 - recon: 34.5247 - kl_loss: 7.6797 - mi: 4.7697 - tc: 2.7035 - dw_kl: 24.6910 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 7/20\n",
      "277/277 [==============================] - 11s 39ms/step - loss: 42.5159 - recon: 34.9087 - kl_loss: 7.6958 - mi: 4.7612 - tc: 2.5933 - dw_kl: 22.9015 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 8/20\n",
      "277/277 [==============================] - 11s 40ms/step - loss: 42.4217 - recon: 34.6800 - kl_loss: 7.7226 - mi: 4.8273 - tc: 2.6446 - dw_kl: 23.3027 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 9/20\n",
      "277/277 [==============================] - 11s 39ms/step - loss: 42.5594 - recon: 34.7413 - kl_loss: 7.7615 - mi: 4.9317 - tc: 2.7235 - dw_kl: 23.2645 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 10/20\n",
      "277/277 [==============================] - 11s 39ms/step - loss: 42.2539 - recon: 33.8914 - kl_loss: 7.7936 - mi: 4.8841 - tc: 2.9137 - dw_kl: 25.2735 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 11/20\n",
      "277/277 [==============================] - 11s 39ms/step - loss: 42.4027 - recon: 34.7247 - kl_loss: 7.8153 - mi: 4.8498 - tc: 2.6909 - dw_kl: 22.7768 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 12/20\n",
      "277/277 [==============================] - 11s 39ms/step - loss: 41.9577 - recon: 33.5852 - kl_loss: 7.8469 - mi: 4.8840 - tc: 2.9437 - dw_kl: 25.2035 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 13/20\n",
      "277/277 [==============================] - 11s 39ms/step - loss: 42.1440 - recon: 33.5968 - kl_loss: 7.8614 - mi: 4.9283 - tc: 2.9464 - dw_kl: 26.0220 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 14/20\n",
      "277/277 [==============================] - 11s 39ms/step - loss: 42.3279 - recon: 34.3058 - kl_loss: 7.8833 - mi: 4.8124 - tc: 2.8494 - dw_kl: 23.9005 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 15/20\n",
      "277/277 [==============================] - 11s 39ms/step - loss: 41.7875 - recon: 33.3860 - kl_loss: 7.8945 - mi: 4.8884 - tc: 2.8942 - dw_kl: 25.5420 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 16/20\n",
      "277/277 [==============================] - 11s 39ms/step - loss: 42.6045 - recon: 34.7500 - kl_loss: 7.8964 - mi: 4.9092 - tc: 2.8820 - dw_kl: 22.8351 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 17/20\n",
      "277/277 [==============================] - 11s 39ms/step - loss: 42.9768 - recon: 35.1939 - kl_loss: 7.9063 - mi: 4.8634 - tc: 2.7436 - dw_kl: 23.0765 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 18/20\n",
      "277/277 [==============================] - 11s 39ms/step - loss: 41.7456 - recon: 33.1497 - kl_loss: 7.9117 - mi: 4.9215 - tc: 3.0185 - dw_kl: 25.9843 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 19/20\n",
      "277/277 [==============================] - 11s 39ms/step - loss: 42.5499 - recon: 34.8127 - kl_loss: 7.9109 - mi: 4.8465 - tc: 2.8601 - dw_kl: 22.3987 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 20/20\n",
      "277/277 [==============================] - 11s 39ms/step - loss: 41.9826 - recon: 33.7345 - kl_loss: 7.9324 - mi: 4.8824 - tc: 2.8243 - dw_kl: 25.0611 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "277/277 [==============================] - 11s 39ms/step\n",
      "6 \t Split: 3013\n",
      "Epoch 1/20\n",
      "228/228 [==============================] - 11s 42ms/step - loss: 11.9446 - recon: 4.6646 - kl_loss: 8.4338 - mi: 3.2469 - tc: 0.8021 - dw_kl: 29.9447 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 2/20\n",
      "228/228 [==============================] - 9s 39ms/step - loss: 10.9626 - recon: 4.8847 - kl_loss: 6.5513 - mi: 3.5180 - tc: 0.8045 - dw_kl: 23.6539 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 3/20\n",
      "228/228 [==============================] - 9s 38ms/step - loss: 10.8994 - recon: 4.5627 - kl_loss: 6.2749 - mi: 3.5842 - tc: 0.7546 - dw_kl: 25.0807 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 4/20\n",
      "228/228 [==============================] - 9s 38ms/step - loss: 10.3367 - recon: 4.5145 - kl_loss: 6.2320 - mi: 3.6148 - tc: 0.6778 - dw_kl: 22.7853 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 5/20\n",
      "228/228 [==============================] - 10s 39ms/step - loss: 10.9398 - recon: 4.3948 - kl_loss: 6.2063 - mi: 3.6565 - tc: 0.7772 - dw_kl: 25.9600 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 6/20\n",
      "228/228 [==============================] - 9s 39ms/step - loss: 10.7198 - recon: 4.8744 - kl_loss: 6.2250 - mi: 3.7052 - tc: 0.8653 - dw_kl: 22.0606 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 7/20\n",
      "228/228 [==============================] - 9s 39ms/step - loss: 10.5597 - recon: 4.3012 - kl_loss: 6.2349 - mi: 4.0527 - tc: 0.7109 - dw_kl: 24.3959 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 8/20\n",
      "228/228 [==============================] - 10s 39ms/step - loss: 10.6953 - recon: 4.5872 - kl_loss: 6.2691 - mi: 3.8798 - tc: 0.5963 - dw_kl: 24.2753 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 9/20\n",
      "228/228 [==============================] - 10s 39ms/step - loss: 10.5813 - recon: 4.0753 - kl_loss: 6.2851 - mi: 3.9300 - tc: 0.7554 - dw_kl: 25.5782 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 10/20\n",
      "228/228 [==============================] - 9s 39ms/step - loss: 10.6897 - recon: 4.3092 - kl_loss: 6.2923 - mi: 3.9297 - tc: 0.8501 - dw_kl: 24.5722 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 11/20\n",
      "228/228 [==============================] - 10s 40ms/step - loss: 10.3524 - recon: 4.2005 - kl_loss: 6.3163 - mi: 3.7831 - tc: 0.4688 - dw_kl: 25.1009 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 12/20\n",
      "228/228 [==============================] - 11s 47ms/step - loss: 10.5246 - recon: 4.2361 - kl_loss: 6.3219 - mi: 3.7988 - tc: 0.7055 - dw_kl: 24.8216 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 13/20\n",
      "228/228 [==============================] - 11s 48ms/step - loss: 10.5595 - recon: 3.9752 - kl_loss: 6.3481 - mi: 3.8953 - tc: 0.9596 - dw_kl: 25.1875 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 14/20\n",
      "228/228 [==============================] - 12s 51ms/step - loss: 10.6506 - recon: 4.2280 - kl_loss: 6.3436 - mi: 3.9750 - tc: 0.7943 - dw_kl: 24.9608 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 15/20\n",
      "228/228 [==============================] - 13s 55ms/step - loss: 10.3086 - recon: 4.3053 - kl_loss: 6.3561 - mi: 4.0242 - tc: 0.6390 - dw_kl: 23.4365 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 16/20\n",
      "228/228 [==============================] - 9s 40ms/step - loss: 10.5925 - recon: 4.0352 - kl_loss: 65.9957 - mi: 3.8619 - tc: 1.0131 - dw_kl: 24.8721 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 17/20\n",
      "228/228 [==============================] - 9s 39ms/step - loss: 10.6818 - recon: 3.9667 - kl_loss: 6.4801 - mi: 4.1061 - tc: 0.9824 - dw_kl: 25.5394 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 18/20\n",
      "228/228 [==============================] - 9s 39ms/step - loss: 10.4904 - recon: 4.2822 - kl_loss: 6.4007 - mi: 3.8643 - tc: 0.7789 - dw_kl: 24.0609 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 19/20\n",
      "228/228 [==============================] - 10s 39ms/step - loss: 10.3931 - recon: 3.9848 - kl_loss: 6.4148 - mi: 4.1714 - tc: 0.7093 - dw_kl: 25.0331 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 20/20\n",
      "228/228 [==============================] - 10s 39ms/step - loss: 10.8377 - recon: 4.3194 - kl_loss: 7.3405 - mi: 3.8386 - tc: 0.8003 - dw_kl: 25.5518 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "228/228 [==============================] - 9s 38ms/step\n",
      "7 \t Split: 6607\n",
      "Epoch 1/20\n",
      "221/221 [==============================] - 10s 41ms/step - loss: 7.8091 - recon: 0.4718 - kl_loss: 12.5024 - mi: 4.3311 - tc: 1.6528 - dw_kl: 25.7438 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 2/20\n",
      "221/221 [==============================] - 8s 38ms/step - loss: 5.6753 - recon: 0.4706 - kl_loss: 7.7900 - mi: 4.9001 - tc: 0.2015 - dw_kl: 20.3172 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 3/20\n",
      "221/221 [==============================] - 9s 39ms/step - loss: 5.2917 - recon: 0.4303 - kl_loss: 7.0700 - mi: 4.6109 - tc: 1.3851 - dw_kl: 14.1557 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 4/20\n",
      "221/221 [==============================] - 9s 39ms/step - loss: 5.1399 - recon: 0.4037 - kl_loss: 6.8994 - mi: 5.0633 - tc: 0.4430 - dw_kl: 16.8453 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 5/20\n",
      "221/221 [==============================] - 9s 39ms/step - loss: 4.4969 - recon: 0.3857 - kl_loss: 6.8507 - mi: 4.6688 - tc: 0.1312 - dw_kl: 15.3627 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 6/20\n",
      "221/221 [==============================] - 9s 39ms/step - loss: 7.7999 - recon: 0.4913 - kl_loss: 6.8728 - mi: 4.3832 - tc: 2.0515 - dw_kl: 23.9538 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 7/20\n",
      "221/221 [==============================] - 9s 39ms/step - loss: 6.0399 - recon: 0.3967 - kl_loss: 6.9053 - mi: 4.8299 - tc: 0.0995 - dw_kl: 22.9884 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 8/20\n",
      "221/221 [==============================] - 9s 39ms/step - loss: 6.8243 - recon: 0.4569 - kl_loss: 6.9468 - mi: 4.8394 - tc: 1.4265 - dw_kl: 21.2915 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 9/20\n",
      "221/221 [==============================] - 9s 39ms/step - loss: 6.5402 - recon: 0.4018 - kl_loss: 6.9840 - mi: 5.3879 - tc: 0.9408 - dw_kl: 21.5409 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 10/20\n",
      "221/221 [==============================] - 9s 39ms/step - loss: 5.1042 - recon: 0.4649 - kl_loss: 7.0209 - mi: 5.3417 - tc: 0.3313 - dw_kl: 16.5297 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000\n",
      "Epoch 11/20\n",
      " 13/221 [>.............................] - ETA: 8s - loss: 60.5495 - recon: 53.1769 - kl_loss: 6.9508 - mi: 4.3864 - tc: 2.7019 - dw_kl: 21.6690 - alpha: 0.2000 - beta: 0.8000 - gamma: 0.2000"
     ]
    }
   ],
   "source": [
    "for i, split in enumerate(splits):\n",
    "    print(i, '\\t', 'Split:', split)\n",
    "    datasets.update({'split': split})\n",
    "\n",
    "    data_train = tfds.load('icentia11k', split=[split])\n",
    "    train = data_train[0].batch(datasets['batch_size']).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    encoder = Encoder(ld)\n",
    "    decoder = Decoder(ld)\n",
    "\n",
    "    model_personalization = TCVAE(encoder, decoder, {'alpha': 0.2, 'beta': 0.4, 'gamma': 0.2}, len(train))\n",
    "    model_personalization.compile(optimizer=RMSprop(learning_rate=0.001))\n",
    "    model_personalization.set_weights(model.weights)\n",
    "    model_personalization.fit(\n",
    "        Helper.data_generator([train]),\n",
    "        steps_per_epoch=len(train),\n",
    "        epochs=20, verbose=1,\n",
    "    )\n",
    "    embeddings, ld = get_embeddings(model_personalization, datasets)\n",
    "    embedding = embeddings[0]\n",
    "    embedding['split'] = split\n",
    "    embedding.to_csv('./embedding_personalization/' + split + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501eb53a-0aeb-4854-89c6-5ccc33f0ab5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63facbe4-8d55-4c5c-a90f-23035f64a0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
